{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/willduan1/article/details/53070777\n",
    "https://www.cnblogs.com/nxld/p/6058782.html\n",
    "https://blog.csdn.net/qq_18254385/article/details/78428887\n",
    "https://blog.csdn.net/dyx810601/article/details/82141789\n",
    "https://www.cnblogs.com/pinard/p/5992719.html\n",
    "https://blog.csdn.net/aliceyangxi1987/article/details/73532651\n",
    "\n",
    "# 机器学习的概念\n",
    "\n",
    "## 有监督和无监督\n",
    "有监督：数据有标签，过程通常是：在已知的训练数据（输入和对应的输出下），训练得到模型；核心是分类，选择不同的分类器，分类后会被直接打上标签，适合独立同分布数据。\n",
    "\n",
    "无监督：不存在老师即数据没有标签，直接拿数据进行建模；存在一个度量描述，学习结果的质量，基于这个度量优化模型参数。核心是聚类，计算密度相似度，先聚类后定性，对训练样本偏移的鲁棒性强，适合非独立同分布数据，可解释性优于有监督模型，扩展性更好。\n",
    "\n",
    "## 泛化能力\n",
    "泛化能力：机器学习算法对未知数据的识别能力，举一反三的能力。\n",
    "在训练开始时，训练集误差和验证集误差都很大，处于欠拟合状态，高偏差低方差。训练不断进行，训练集误差不断减小，验证集先减小后增大。验证集误差变大即进入过拟合状态，此时低偏差高方差。\n",
    "\n",
    "## 过拟合和欠拟合及解决\n",
    "过拟合：模型的复杂度要高于实际的问题，训练过头。机器只是记住了训练数据:在训练数据上表现好，在未知数据上表现差。\n",
    "\n",
    "解决方法：\n",
    "1.\t重新清洗数据，过拟合可能是数据不纯导致的。\n",
    "2.\t增加训练数据量，两者是相对的。\n",
    "3.\t减少参数 。\n",
    "\n",
    "$\\triangleright$损失函数引入正则项。参数过多，惩罚越重，对应损失函数就会变大，而我们的目标是最小化loss 。\n",
    "\n",
    "$\\triangleright$对于一些特定的模型采取剪枝或dropout,BN。\n",
    "4. 重采样方法和验证集方法 常用K折交叉检验：在训练数据的子集上训练和测试模型k次，基于指标优化。\n",
    "5.\t换个模型，模型不适合这个任务。\n",
    "\n",
    "欠拟合：模型的复杂度较低，训练不够。没有学习到数据背后的规律。在训练数据和未知数据上表现都很差。\n",
    "解决方法：不断训练至过拟合，增加模型复杂度（增加特征，减少正则项）。\n",
    "## 交叉验证\n",
    "交叉验证:\n",
    "对样本数据进行切分为k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性,然后每次用k-1个子集的并集做为训练集，余下的子集做为测试集样本.打乱可以得到多组不同的训练集和测试集;某次训练集中的样本在下次可能成为测试集中的样本，即所谓“交叉”。常用方法：留出法，k折交叉检验，bootstrapping 。对应的python库有sklearn。\n",
    "# 线性回归的原理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义\n",
    "定义线性方程组 $y=\\sum_{i=1}^{m} x_i  w_i$,写成向量的形式 $Y=XW$;\n",
    "\n",
    "其中X为样本数据，W为线性回归模型参数，Y是期望数据,m表示样本数量，其中X,Y已知；\n",
    "\n",
    "问题：寻找合适的W,合适的尺度。\n",
    "\n",
    "尺度即损失函数，用于衡量训练过程中模型产生和期望产出的差距，并根据该目标找到合适的优化方法接近该目标。\n",
    "\n",
    "## 损失函数\n",
    "损失函数的别称，代价函数，目标函数\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T02:40:19.700889Z",
     "start_time": "2019-02-28T02:40:19.695899Z"
    }
   },
   "source": [
    "$y=xw+ \\varepsilon $,即$y=\\widehat{y}+ \\varepsilon $\n",
    "\n",
    "y是期望输出，$\\widehat{y}$是模型输出，$\\varepsilon $是误差；在数据量足够大的情况下，误差$\\varepsilon $满足$(\\mu,\\sigma)$的高斯分布。方程有截距，此时$\\mu=0$。\n",
    "\n",
    "单个样本的误差即 $P(\\varepsilon^i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(\\varepsilon^i)^2}{2\\sigma^2})$ ,$\\varepsilon$用$y-\\widehat{y}$代换，得到对应条件概率：$P(y^i\\mid x^i,w^i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^i-\\widehat{y})^2}{2\\sigma^2})$。\n",
    "\n",
    "由于样本是独立同分布，此时计算对数似然估计$L(W)=log \\prod^m_{i=1}P(y^i\\mid x^i,w^i)$\n",
    "\n",
    "$L=\\sum^m_{i=1}log \\frac{1}{\\sqrt{2\\pi}\\sigma}exp({-\\frac{(y^i-\\widehat{y})^2}{2\\sigma^2}})$引入log累乘成累加\n",
    "\n",
    "$L=log \\frac{m}{\\sqrt{2\\pi}+\\sigma}+\\sum^m_{i=1}log[exp({-\\frac{(y^i-\\widehat{y})^2}{2\\sigma^2}})]$进一步化简\n",
    "\n",
    "$L=log \\frac{m}{\\sqrt{2\\pi}+\\sigma}-\\frac{1}{2\\sigma^2}\\sum^m_{i=1}log[exp((y^i-\\widehat{y})^2)]$\n",
    "\n",
    "$L=log \\frac{m}{\\sqrt{2\\pi}+\\sigma}-\\frac{1}{2\\sigma^2}\\sum^m_{i=1}(y^i-\\widehat{y})^2$，\n",
    "\n",
    "令$k_1=log \\frac{m}{\\sqrt{2\\pi}+\\sigma}$,$k_2=\\frac{1}{\\sigma^2}$,得到$L=k_1-k_2\\frac{1}{2}\\sum^m_{i=1}(y^i-\\widehat{y})^2$\n",
    "\n",
    "可以得出以下结论：$k_1$,$k_2$是正常数，训练目标是最大化对数似然估计，即$\\max L$,换句话说就是最小化$\\frac{1}{2}\\sum^m_{i=1}(y^i-\\widehat{y})^2$,\n",
    "\n",
    "得到损失函数$L=\\frac{1}{2}\\sum^m_{i=1}(y^i-\\widehat{y})^2$=$\\frac{1}{2}\\sum^m_{i=1}(y^i- x_i  w_i)^2$\n",
    "\n",
    "小节：根据中心极限定理估计误差分布，$\\varepsilon$用$y-\\widehat{y}$代换，计算对数似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化方法\n",
    "两类：最小二乘法和梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数写成向量形式$L=\\frac{1}{2}(Y-XW)^T(Y-XW)$\n",
    "\n",
    "$L=\\frac{1}{2}(Y^T-W^TX^T)(Y-XW)$,\n",
    "\n",
    "$L=\\frac{1}{2}(Y^TY-Y^TXW-W^TX^TY+W^TX^TXW)$\n",
    "\n",
    "$L=\\frac{1}{2}[Y^TY-Y^TXW-(Y^TXW)^T+(XW)^TXW]$\n",
    "\n",
    "求$L$最小值，$L'(w)=0$即可，此时$L'=-Y^TX-(Y^TX)^T+X^T(XW)+(XW)^TX=0$\n",
    "\n",
    "即$Y^TX=X^T(XW)$,$W=(X^TX)^{-1}YTX$\n",
    "右边$X,Y$是已知的，可以直接计算的前提是$X^TX$可逆，引入参数$\\lambda$,可以保证$(X^TX+\\lambda I)^{-1}$一直存在，这种方法叫岭回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度方向是函数增长最快的方向，可知梯度反方向下降最快\n",
    "\n",
    "$L(W)$，通过更新$W$值，损失函数沿梯度反向降低。$W^{i}=W^{i}-\\alpha\\frac{\\partial L}{\\partial W}$,$\\alpha$是学习率,控制步长。\n",
    "\n",
    "怎么求梯度？求导\n",
    "这里损失函数记为L,回归方程记为f,样本数量为m,参数数量为n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机梯度下降法\n",
    "一个样本更新一次参数，$L=\\frac{1}{2}[f(W)-y]^2$,由于W是向量，计算偏导，即\n",
    "\n",
    "$\\frac{\\partial f(W)}{\\partial W_i}=[f(W)-y]\\frac{\\partial f(W)}{\\partial W_i}=[f(W)-y]\\frac{\\partial (W_0X_0+...+W_nX_n)}{\\partial W_i}=[f(W)-y]X_i$\n",
    "\n",
    "$W^{i}=W^{i}-\\alpha[f(W)-y]X_i$\n",
    "\n",
    "同样为了减少过拟合，引入不同的正则项\n",
    "\n",
    "L1正则（Lasso回归）：$\\lambda\\sum^n_{j=1}\\mid W_j\\mid$\n",
    "\n",
    "L2正则（Ridge回归）：$\\lambda\\sum^n_{i=j}W_j^2$\n",
    "\n",
    "结合以上（Elastic Net 回归）：$\\rho\\sum^n_{j=1}\\mid W_j\\mid+(1-\\rho)\\sum^n_{i=j}W_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 批量梯度下降\n",
    "遍历所有样本后再更新参数$L=\\frac{1}{2}\\sum^m_{i=1}[f(W)-y]^2$\n",
    "\n",
    "$W^{i}=W^{i}-\\alpha\\sum^m_{i=1}[f(W)-y]X_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数（见2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法，牛顿法和拟牛顿法\n",
    "## 梯度下降法\n",
    "\n",
    "问题：求解最优化问题$min_xf(x)$,其中x是优化变量，f是目标函数\n",
    "\n",
    "梯度下降法原理：从初始点$x_0$开始，利用规则移动点，最终梯度值为0。\n",
    "\n",
    "根据泰勒展开公式，忽略二次项$f(x)=f(x^{(k)})+g^T_k(x-x^{(k)})$,\n",
    "\n",
    "移项$f(x)-f(x^{(k)})=g^T_k(x-x^{(k)})<0$,保证每次迭代$f(x)$值都在减小，保证右边项小于0,即$\\mid\\mid g^T_k\\mid\\mid \\mid\\mid(x-x^{(k)})\\mid\\mid cos(\\theta)<0$,\n",
    "\n",
    "有$cos(\\theta)<0$,又有$cos(\\theta)\\ge-1$,当$\\theta=180$度时，变化最大，$x^{(k+1)}-x^{(k)}=-\\lambda_k g^T_k$,\n",
    "\n",
    "训练过程：\n",
    "\n",
    "目标函数$f(x)$,梯度函数$g(x)$,计算精度$\\epsilon$,极小点$x^*$\n",
    "\n",
    "1.初始化$x^{(0)}$,k=0\n",
    "\n",
    "2.计算$f(x^{(0)})$\n",
    "\n",
    "3.计算梯度$g_k=g(x^{(k)})$,判断当$g_k<\\epsilon$时停止迭代，令$x^*=x^{(k)}$;否则令$p_k=-g(x^{(k)})$,求$\\lambda_k$\n",
    "\n",
    "$f(x^{(k)}+\\lambda_k p_k)=min_{\\lambda\\ge0}f(x^{(k)}+\\lambda p_k)$,\n",
    "\n",
    "4.$x^{(k+1)}=x^{(k)}+\\lambda_k p_k$,计算$f(x^{(k+1)})$\n",
    "\n",
    "当$\\mid\\mid f(x^{(k+1)})-f(x^{(k)})\\mid\\mid<\\epsilon$或$\\mid\\mid x^{(k+1)}-x^{(k)}\\mid\\mid<\\epsilon$,停止迭代，令$x^*=x^{(k)}$\n",
    "\n",
    "5.否则，令$k=k+1$,跳回3\n",
    "\n",
    "\n",
    "## 牛顿法\n",
    "\n",
    "梯度下降法:$x^{(k+1)}=x^{(k)}-\\lambda_k g^T_k$,$\\lambda_k$要经过一维搜索\n",
    "\n",
    "牛顿法：$x^{(k+1)}=x^{(k)}-H^{-1}_{k} g^T_k$,这里直接用Hessian矩阵的逆替代$\\lambda_k$\n",
    "\n",
    "梯度下降法原理：根据泰勒展开公式，忽略三次项$f(x)=f(x^{(k)})+g^T_k(x-x^{(k)})+\\frac{1}{2!}(x-x^{(k)})^TH(x^{(k)})(x-x^{(k)})$,,其中\n",
    "\n",
    "$H(x^{(k)})=[\\frac{\\partial^2 f}{\\partial_{x_i}\\partial_{y_j}}]_{nxn}$\n",
    "\n",
    "令$ g^T_{k+1}=0$，$g^T_k+H(x^{(k)})(x-x^{(k)})=0$,++++++++++++++++++++++++(0)\n",
    "\n",
    "$x^{(k+1)}=x^{(k)}-H^{-1}_{k} g^T_k$\n",
    "\n",
    "训练过程：\n",
    "目标函数$f(x)$,梯度函数$g(x)$,Hessian矩阵$H(x)$,计算精度$\\epsilon$,极小点$x^*$\n",
    "\n",
    "1.初始化$x^{(0)}$,k=0\n",
    "\n",
    "2.计算梯度$g_k=g(x^{(k)})$,判断当$g_k<\\epsilon$时停止迭代，令$x^*=x^{(k)}$;\n",
    "\n",
    "3.计算$H_k=H(x^{(k)})$,计算$p_k$\n",
    "\n",
    "$p_k=-H^{-1}_kg_k$+++++++++++++++++++++++++++(1)\n",
    "\n",
    "4.$x^{(k+1)}=x^{(k)}+ p_k$\n",
    "\n",
    "当$\\mid\\mid f(x^{(k+1)})-f(x^{(k)})\\mid\\mid<\\epsilon$或$\\mid\\mid x^{(k+1)}-x^{(k)}\\mid\\mid<\\epsilon$,停止迭代，令$x^*=x^{(k)}$\n",
    "\n",
    "5.否则，令$k=k+1$,跳回3\n",
    "\n",
    "## 拟牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心：$H^{-1}$不好求,用新的n阶矩阵$G_k=G(x^{(k)})$去近似"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "牛顿法中H矩阵的限制条件，用于近似的G矩阵也应满足。\n",
    "\n",
    "4.2节+++++(0)区域变换得到$ g_{k+1}-g_k=H(x^{(k)})(x-x^{(k)})=0$,方便表示令$y_k=g_{k+1}-g_k$,$\\delta_k=x^{(k+1)}-x^{(k)}$\n",
    "\n",
    "$y_k=H_k\\delta_k$,如果$H_k$是正定，随迭代进行值总在下降，此时$x^{(k+1)}-x^{(k)}= \\lambda p_k$\n",
    "\n",
    "带入泰勒公式$f(x)=f(x^k)+g^{T}_k(\\lambda p_k)=f(x^k)-\\lambda g^{T}_kH^{-1}_kg_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H矩阵的限制条件:\n",
    "\n",
    "1.正定\n",
    "\n",
    "2.$y_k=H_k\\delta_k$或$H^{-1}y_k=\\delta_k$\n",
    "\n",
    "复习下牛顿法：计算梯度$g_k=g(x^{(k)})$，$H_k=H(x^{(k)})$，计算$p_k$，$p_k=-H^{-1}_kg_k$，$x^{(k+1)}=x^{(k)}+ p_k$\n",
    "\n",
    "$G_k$近似$H^{-1}$，满足以上条件；当然也可以用$B_k$拟合$H_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFP算法\n",
    "$G_k$近似$H^{-1}$,\n",
    "\n",
    "假设$G_{k+1}=G_{k}+\\Delta G$,\n",
    "\n",
    "构造$\\Delta G=\\alpha uu^T+\\beta vv^t$,\n",
    "\n",
    "$\\delta_k=G_{k+1}y_k=G_{k}y_k+\\Delta Gy_k=G_{k}y_k+\\alpha uu^Ty_k+\\beta vv^ty_k=G_{k}y_k+u(\\alpha u^Ty_k)+v(\\beta v^ty_k)$\n",
    "\n",
    "令$\\alpha u^Ty_k=1,\\beta v^ty_k=-1$,\n",
    "\n",
    "$\\alpha=\\frac{1}{u^Ty_k},\\beta=-\\frac{1}{v^Ty_k} $,\n",
    "\n",
    "此时$\\delta_k-G_{k}y_k=u-v$,令$u=\\delta_k,v=G_{k}y_k$,\n",
    "\n",
    "$\\alpha=\\frac{1}{\\delta^T_ky_k},\\beta=-\\frac{1}{y^T_kG^T_{k}y_k}=-\\frac{1}{y^T_kG_{k}y_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta G=\\frac{\\delta_k\\delta^{T}_k}{\\delta^T_ky_k}-\\frac{G_{k}y_ky^T_kG^T_{k}}{y^T_kG_{k}y_k}$\n",
    "\n",
    "$G_{k+1}=G_{k}+\\frac{\\delta_k\\delta^{T}_k}{\\delta^T_ky_k}-\\frac{G_{k}y_ky^T_kG^T_{k}}{y^T_kG_{k}y_k}$,\n",
    "\n",
    "$y_k=g_{k+1}-g_k$,$\\delta_k=x^{(k+1)}-x^{(k)}$\n",
    "\n",
    "\n",
    "训练过程：\n",
    "\n",
    "目标函数$f(x)$,梯度函数$g(x)$,Hessian矩阵$H(x)$,计算精度$\\epsilon$,极小点$x^*$\n",
    "\n",
    "1.初始化$x^{(0)}$,k=0,取$G_0$为正定对称矩阵\n",
    "\n",
    "2.计算梯度$g_k=g(x^{(k)})$,判断当$g_k<\\epsilon$时停止迭代，令$x^*=x^{(k)}$;否则3\n",
    "\n",
    "3.计算$p_k$，$p_k=-G_kg_k$，求$\\lambda_k$，\n",
    "\n",
    "$f(x^{(k)}+\\lambda_k p_k)=min_{\\lambda\\ge0}f(x^{(k)}+\\lambda p_k)$,\n",
    "\n",
    "4.$x^{(k+1)}=x^{(k)}+\\lambda_k p_k$\n",
    "\n",
    "5.计算梯度$g_{k+1}=g(x^{(k+1)})$,判断当$g_{k+1}<\\epsilon$时停止迭代，令$x^*=x^{(k+1)}$;\n",
    "\n",
    "否则，$G_{k+1}=G_{k}+\\frac{\\delta_k\\delta^{T}_k}{\\delta^T_ky_k}-\\frac{G_{k}y_ky^T_kG^T_{k}}{y^T_kG_{k}y_k}$,\n",
    "\n",
    "$y_k=g_{k+1}-g_k$,$\\delta_k=x^{(k+1)}-x^{(k)}$\n",
    "\n",
    "6.令$k=k+1$,跳回3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS算法\n",
    "用$B_k$拟合$H_k$\n",
    "证明方法同上结果是$\\delta_k$和$y_k$互换位置，不证明直接给出解果\n",
    "$B_{k+1}=B_{k}+\\frac{y_ky^{T}_k}{y^T_k\\delta_k}-\\frac{B_{k}\\delta_k\\delta^T_kB^T_{k}}{\\delta^T_kB_{k}\\delta_k}$,\n",
    "\n",
    "$y_k=g_{k+1}-g_k$,$\\delta_k=x^{(k+1)}-x^{(k)}$\n",
    "\n",
    "训练过程：\n",
    "\n",
    "目标函数$f(x)$,梯度函数$g(x)$,Hessian矩阵$H(x)$,计算精度$\\epsilon$,极小点$x^*$\n",
    "\n",
    "1.初始化$x^{(0)}$,k=0,取$B_0$为正定对称矩阵\n",
    "\n",
    "2.计算梯度$g_k=g(x^{(k)})$,判断当$g_k<\\epsilon$时停止迭代，令$x^*=x^{(k)}$;否则3\n",
    "\n",
    "3.计算$p_k$，$B_kp_k=-g_k$，求$\\lambda_k$，\n",
    "\n",
    "$f(x^{(k)}+\\lambda_k p_k)=min_{\\lambda\\ge0}f(x^{(k)}+\\lambda p_k)$,\n",
    "\n",
    "4.$x^{(k+1)}=x^{(k)}+\\lambda_k p_k$\n",
    "\n",
    "5.计算梯度$g_{k+1}=g(x^{(k+1)})$,判断当$g_{k+1}<\\epsilon$时停止迭代，令$x^*=x^{(k+1)}$;\n",
    "\n",
    "否则，$B_{k+1}=B_{k}+\\frac{y_ky^{T}_k}{y^T_k\\delta_k}-\\frac{B_{k}\\delta_k\\delta^T_kB^T_{k}}{\\delta^T_kB_{k}\\delta_k}$,\n",
    "\n",
    "$y_k=g_{k+1}-g_k$,$\\delta_k=x^{(k+1)}-x^{(k)}$\n",
    "\n",
    "6.令$k=k+1$,跳回3\n",
    "\n",
    "### 其他算法\n",
    "\n",
    "Broyden算法：$G_k+1=\\alpha G^{DFP}+(1-\\alpha)G^{BFGS}$,各取上面算法一部分\n",
    "\n",
    "#B.31 不会推2333\n",
    "L-BFGS 就是存储过程如果碰到内存不过根据小稚数据量，丢弃最早生成的$y_k,\\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的评估指标\n",
    "统一使用预测值f,期望值y\n",
    "## MAE(平均绝对误差）\n",
    "$MAE=\\frac{1}{m}\\sum^m_{i=1}\\mid y-f\\mid$\n",
    "误差越小，模型越好\n",
    "## MSE(均方误差）\n",
    "$MSE=\\frac{1}{m}\\sum^m_{i=1}(y-f)^2$\n",
    "就是之前定义的损失函数\n",
    "## RMSE(均方根误差）\n",
    "$RMSE=\\sqrt{\\frac{1}{m}\\sum^m_{i=1}(y-f)^2}$均方误差加个根号\n",
    "## R平方\n",
    "$R^2=1-\\frac{(y-f)^2}{(y-\\overline{y})^2}$\n",
    "\n",
    "分子表示模型预测误差，分母表示原始数据的离散程度，相除是为了消除原始离散的影响\n",
    "\n",
    "$R^2$越接近1，模型越好，当$y=f$时全部预测正确，$R^2=1$；当$f=\\overline{y}$时模型胡乱预测，结果为0；当$R^2<0$时，表示x,y没有线性关系。但是随样本量增加，$R^2$也会增加\n",
    "## Adjusted R平方\n",
    "$R_{adjusted}=1-\\frac{(1-R^2)(n-1)}{n-p-1}$\n",
    "n,p分别表示样本量，特征量；放在分母就是为了消除两者的影响\n",
    "## MAPE\n",
    "$MAPE=\\frac{100}{n}\\sum^n_{i=1}\\mid \\frac{y_i-f_i}{y_i}\\mid$\n",
    "考虑了误差与期望值的距离\n",
    "## RMSPE\n",
    "$RMSPE=\\sqrt{\\frac{1}{n}\\sum^n_{i=1}(\\frac{y_i-f_i}{y_i})^2}$ kaggle使用的一套指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN参数详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.model = LinearRegression(fit_intercept=True, normalize=False,\n",
    "                         copy_X=True, n_jobs=1)\n",
    "-----fit_intercept：是否计算截距。\n",
    "-----normalize： 当fit_intercept=True时，回归前的回归系数X减去平均值并除以l2-范数进行归一化。\n",
    "-----copy_X：是否对X数组进行复制,默认为True\n",
    "-----n_jobs：指定线程数\n",
    "\n",
    "2.fit(X,y,sample_weight=None)#用于训练模型，返回值：实例\n",
    "-----X:形如[样本数，特征数]的训练数据\n",
    "-----y:形如[样本数，标签数]的训练标签\n",
    "-----样本权重：每个样本有独立的权重形如[样本数]\n",
    "\n",
    "3.get_param(deep=True):获取当前模型及子项目的参数\n",
    "-----deep布尔值：True,返回参数\n",
    "-----返回值：参数名到对应值的映射字符串\n",
    "\n",
    "\n",
    "4.predict(X)\n",
    "-----X:形如[样本数，特征数]的数据\n",
    "-----返回预测值\n",
    "\n",
    "5.score(X, y, sample_weight=None)\n",
    "-----X:形如[样本数，特征数]的测试数据\n",
    "-----y:形如[样本数，标签数]的数据的真实值\n",
    "-----样本权重：每个样本有独立的权重形如[样本数]\n",
    "-----返回评价指标R^2\n",
    "\n",
    "R^2=1-U/V,其中u=((y_true - y_pred) ** 2).sum(),v=((y_true - y_true.mean()) ** 2).sum()\n",
    "\n",
    "R^2的最优值是1；如果模型表现力很差，R^2可以是负值。常量模型在预测过程中，忽视输入特征，此时R^2=0\n",
    "\n",
    "6.set_params(** params)\n",
    "-----设置参数\n",
    "-----返回实例\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
